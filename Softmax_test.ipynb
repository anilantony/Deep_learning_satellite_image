{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network form strach using sogmoid and softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define neural network with multiple class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)Lets initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_of_parameters(list_nodes):\n",
    "    np.random.seed(3) #Random seed is change for example set back to 3\n",
    "    Number_of_layer=len(list_nodes)\n",
    "    \n",
    "    parameters={}\n",
    "    \n",
    "    \n",
    "    for l in range(1,Number_of_layer): #to make one less parameter matix than number of layers and match the number W1,W2,b1,b2 etc\n",
    "        \n",
    "        parameters[\"W\"+ str(l)]=np.random.randn(list_nodes[l],list_nodes[l-1])*0.01\n",
    "        parameters[\"b\"+ str(l)]=np.zeros((list_nodes[l],1))\n",
    "        \n",
    "        \n",
    "        assert ( parameters[\"W\"+str(l)].shape == (list_nodes[l],list_nodes[l-1]))\n",
    "        assert (parameters[\"b\"+str(l)].shape==(list_nodes[l],1))\n",
    "        \n",
    "    return  parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Now lets move towards the forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X,W,b):\n",
    "    \n",
    "    Z=np.dot(W,X)+b\n",
    "    \n",
    "    assert (Z.shape== (W.shape[0],X.shape[1]))\n",
    "    storage_linear_forward=(X,W,b)\n",
    "    \n",
    "    return Z,storage_linear_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    storage_sigmoid = Z\n",
    "    \n",
    "    return A, storage_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z)\n",
    "    A=expZ / expZ.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    storage_softmax=Z\n",
    "    \n",
    "    return A, storage_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_with_activation(X,W,b,activation_type):\n",
    "    \n",
    "    if activation_type== \"sigmoid\":\n",
    "        Z,storage_linear_forward= linear_forward(X,W,b)\n",
    "        A,storage_Z=sigmoid(Z)\n",
    "        \n",
    "    elif activation_type == \"softmax\":\n",
    "        Z,storage_linear_forward=linear_forward(X,W,b)\n",
    "        A,storage_Z=softmax(Z)\n",
    "        \n",
    "    assert (A.shape==(Z.shape))\n",
    "    \n",
    "    storage_linear_forward_with_activation=(storage_linear_forward,storage_Z ) #to store X,W,b,Z \n",
    "    \n",
    "    return A,storage_linear_forward_with_activation   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define L-layer forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_for_n_layers(X,parameters):\n",
    "    \n",
    "    Number_of_layer=len(parameters)//2\n",
    "    \n",
    "    storage_forward_propagation_for_all_layers=[]\n",
    "    A=X\n",
    "    \n",
    "    for l in range(1,Number_of_layer):\n",
    "        A_previous= A\n",
    "        \n",
    "        A,storage_linear_forward_with_activation_sigmoid=linear_forward_with_activation(A_previous,\n",
    "                            parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],activation_type=\"sigmoid\")\n",
    "       \n",
    "        storage_forward_propagation_for_all_layers.append(storage_linear_forward_with_activation_sigmoid) #to store X,W,b,Z \n",
    "        \n",
    "        \n",
    "        \n",
    "    A_last,storage_linear_forward_with_activation_softmax=linear_forward_with_activation(A ,\n",
    "                 parameters[\"W\"+str(Number_of_layer)],parameters[\"b\"+str(Number_of_layer)],activation_type=\"softmax\")\n",
    "    \n",
    "\n",
    "    storage_forward_propagation_for_all_layers.append(storage_linear_forward_with_activation_softmax)\n",
    "                                                                                             \n",
    "    assert(A_last.shape==(parameters[\"W\"+str(Number_of_layer)].shape[0],X.shape[1]))\n",
    "    \n",
    "    return A_last, storage_forward_propagation_for_all_layers    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)Now lets calculate the cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_cal(A_last,Y):\n",
    "    cost = - np.sum(np.multiply(np.log(A_last),Y))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert (cost.shape==())\n",
    "    \n",
    "    return cost    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Lets do the backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, storage_linear_forward):\n",
    "  \n",
    "    A_previous, W, b = storage_linear_forward\n",
    "    m = A_previous.shape[1]\n",
    "\n",
    "    \n",
    "    dW = dZ @ A_previous.T\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_previous = W.T @ dZ\n",
    "   \n",
    "    \n",
    "    assert (dA_previous.shape == A_previous.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, storage_Z ):\n",
    "    \n",
    "    Z = storage_Z \n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA,storage_Z ):\n",
    "    Z = storage_Z\n",
    "    dZ =dA * 1 #(From the derivatives of cost entropy and softmax activation function)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, storage_linear_forward_with_activation, activation_type):\n",
    "    \n",
    "    storage_linear_forward,storage_Z=storage_linear_forward_with_activation\n",
    "    \n",
    "    if activation_type == \"softmax\":\n",
    "        \n",
    "        dZ = softmax_backward(dA,storage_Z )\n",
    "        dA_previous, dW, db = linear_backward(dZ, storage_linear_forward)\n",
    "        \n",
    "    elif activation_type == \"sigmoid\":\n",
    "       \n",
    "        dZ = sigmoid_backward(dA, storage_Z)\n",
    "        dA_previous, dW, db = linear_backward(dZ, storage_linear_forward)\n",
    "\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(A_last, Y, storage_forward_propagation_for_all_layers  ):\n",
    "\n",
    "    gradients = {}\n",
    "    Number_of_layer= len( storage_forward_propagation_for_all_layers ) # the number of layers\n",
    "    m = A_last.shape[1]\n",
    "    Y = Y.reshape(A_last.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "  \n",
    "    dA_last = A_last-Y\n",
    "    \n",
    "    current_storage = storage_forward_propagation_for_all_layers[Number_of_layer-1]\n",
    "    gradients[\"dA\" + str(Number_of_layer-1)], gradients[\"dW\" + str(Number_of_layer)], gradients[\"db\" + str(Number_of_layer)] = linear_activation_backward(dA_last, current_storage, \"softmax\")\n",
    "    \n",
    "#     for relu layers\n",
    "    for l in reversed(range(Number_of_layer-1)):\n",
    "        \n",
    "        current_storage =  storage_forward_propagation_for_all_layers [l]\n",
    "        dA_previous_temp, dW_temp, db_temp = linear_activation_backward(gradients[\"dA\" + str(l + 1)], current_storage, \"sigmoid\")\n",
    "        gradients[\"dA\" + str(l)] = dA_previous_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "      \n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Now Lets update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * gradients[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * gradients[\"db\" + str(l + 1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)Now lets merge this all and make a single funtion for L layer Nerual Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_neural_network_model(X, Y, list_nodes, learning_rate = 0.0075, iterations = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "    \n",
    "    #step:1\n",
    "    parameters =initialization_of_parameters(list_nodes)\n",
    "   \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0,iterations):\n",
    "\n",
    "        # Step 2: Forward propagation: linear to sigmoid and linear to softmax\n",
    "        A_last, storage_forward_propagation_for_all_layers = forward_propagation_for_n_layers(X,parameters)\n",
    "    \n",
    "        # step 3 : Calculation of cost.\n",
    "        cost = cost_cal(A_last,Y)\n",
    "    \n",
    "        # step 4: Backward propagation.\n",
    "        gradients = L_model_backward(A_last, Y, storage_forward_propagation_for_all_layers )\n",
    "    \n",
    "        #step 5: Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Cost every 100 iterations\n",
    "        if print_cost and i % 200 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 200 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Predict with updated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):\n",
    " # Forward propagation\n",
    "    probability, param_storage = forward_propagation_for_n_layers(X,parameters)\n",
    "    return probability, param_storage  #gives last layer output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7)Lets check the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(probability,true_class):\n",
    "    \n",
    "    print(f\"The size of output probability :{probability.size} \")\n",
    "    unique_probability=np.unique(probability)\n",
    "    print(f\"The size of unique output probability :{unique_probability.size} \")\n",
    "    print(f\"The size of true class :{true_class.size} \")\n",
    "    \n",
    "    predicted_class=(probability== probability.max(axis=0)).astype(int)\n",
    "    label=np.array([1,2,3]).reshape(3,1)\n",
    "    multiply_1=label* predicted_class\n",
    "    multiply_2=label*true_class\n",
    "    \n",
    "    final_labels_predicted=np.max(multiply_1,axis=0)\n",
    "    final_labels_true=np.max(multiply_2,axis=0)\n",
    "    \n",
    "    diff=(final_labels_predicted-final_labels_true)\n",
    "    count_diff=np.sum(diff)\n",
    "    \n",
    "    m=true_class.shape[1]\n",
    "    \n",
    "    Accuracy= (count_diff/m)*100\n",
    "    \n",
    "    print(f\"Accuracy of model is {Accuracy}%\")\n",
    "    return final_labels_predicted, final_labels_true  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features(X) and true label (Y) (one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
